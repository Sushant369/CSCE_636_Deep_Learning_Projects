{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWAMYd83J95x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ae66fc-aa67-4622-e3ee-0af287991ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "piTrain=pickle.load(open('/content/drive/MyDrive/DL_Project3/DS_5_train_input_prefixList','rb'))\n",
        "nwTrain=pickle.load(open('/content/drive/MyDrive/DL_Project3/DS_5_train_input_nextWord','rb'))\n",
        "\n",
        "train_full=pickle.load(open('/content/drive/MyDrive/DL_Project3/DS_5_train_input','rb'))"
      ],
      "metadata": {
        "id": "z97IjKzNMJbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8BYCcUFFE8Z",
        "outputId": "2145f933-4c83-46f9-c432-56976793cde4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'k', 'b', 'd', 'a', 'j', 'b', 'f', 'a', 'f', 'a', 'k', 'a', 'j', 'c', 'f', 'b', 'f', 'b', 'e', 'b', 'd', 'a', 'e', 'a', 'd', 'c', 'e', 'a', 'g', 'a', 'f', 'a', 'e', 'a', 'g', 'a', 'j', 'c', 'g', 'c', 'f', 'b', 'e', 'a', 'k', 'c', 'e']\n"
          ]
        }
      ],
      "source": [
        "def max_seq_length(data):\n",
        "  max_len=0\n",
        "  unique_letters=[]\n",
        "  for text in data:\n",
        "    # print(text)\n",
        "    text_letters=text\n",
        "    unique_letters+=text_letters\n",
        "    if len(text_letters)>max_len:\n",
        "      max_len=len(text_letters)\n",
        "      if max_len==48:\n",
        "        print(text)\n",
        "  vocab_list=list(set(unique_letters))\n",
        "  vocab_size=len(vocab_list)\n",
        "  return max_len,vocab_list,vocab_size\n",
        "max_train_ip_len,train_vocab_ip_list,train_vocab_ip_size=max_seq_length(piTrain)\n",
        "max_train_ip_len,train_vocab_ip_list,train_vocab_ip_size=max_seq_length(train_full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_ix9xk_FLOw"
      },
      "outputs": [],
      "source": [
        "piTrain_str=[]\n",
        "for i in piTrain:\n",
        "  piTrain_str.append([\" \".join(i)])\n",
        "\n",
        "text_pairs=[]\n",
        "text_data_total=[]\n",
        "for i in range(len(piTrain_str)):\n",
        "  text_pairs.append((piTrain_str[i], nwTrain[i]))\n",
        "  text_data_total.append(piTrain_str[i][0]+\" \"+\n",
        "                         nwTrain[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE7U8nn4UPjn"
      },
      "outputs": [],
      "source": [
        "num_val_samples = int(0.15 * len(text_data_total))\n",
        "num_train_samples =  len(text_data_total) - 2 * num_val_samples\n",
        "train_data_total = text_pairs[:num_train_samples]\n",
        "val_data_total = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "sequence_length = 50\n",
        "vocab_size = 12\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "train_input_texts = [val[0] for val in text_pairs]\n",
        "text_vectorization.adapt(train_input_texts)\n",
        "\n",
        "def format_dataset(prefix, nextword):\n",
        "    prefix = text_vectorization(prefix)\n",
        "    nextword = text_vectorization(nextword)\n",
        "    return (prefix,nextword)"
      ],
      "metadata": {
        "id": "yuchLomqMOfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgNjWbYyFV3f"
      },
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "def make_dataset(pairs):\n",
        "    input_texts, target_texts = zip(*pairs)\n",
        "    input_texts = list(input_texts)\n",
        "    target_texts = list(target_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_texts, target_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "dataset1 = make_dataset(train_data_total)\n",
        "dataset_validation = make_dataset(val_data_total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz2Zx0YCF2Qn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LayerNormalization, Dense, Layer, Attention\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        self.attention_3 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=4*embed_dim)\n",
        "\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.layernorm_4 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        \n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "\n",
        "        attention_output_3 = self.attention_3(\n",
        "            attention_output_2,\n",
        "            attention_output_2\n",
        "        )\n",
        "        attention_output_3 = self.layernorm_3(\n",
        "            attention_output_3 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(attention_output_3)\n",
        "        return self.layernorm_4(attention_output_3 + proj_output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "YYBTArFFMdDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xgTtC__F5Go"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "import keras\n",
        "embed_dim = 20\n",
        "latent_dim = 256\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n",
        "idx_token = dict(enumerate(text_vectorization.get_vocabulary()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awrCWSRbGC4p",
        "outputId": "827e8206-8907-44f9-d2a5-b4065739121b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "110/110 [==============================] - 14s 91ms/step - loss: 0.1676 - accuracy: 0.9556 - val_loss: 0.0826 - val_accuracy: 0.9716\n",
            "Epoch 2/250\n",
            "110/110 [==============================] - 8s 74ms/step - loss: 0.0810 - accuracy: 0.9698 - val_loss: 0.0799 - val_accuracy: 0.9716\n",
            "Epoch 3/250\n",
            "110/110 [==============================] - 8s 76ms/step - loss: 0.0799 - accuracy: 0.9699 - val_loss: 0.0797 - val_accuracy: 0.9716\n",
            "Epoch 4/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0797 - accuracy: 0.9699 - val_loss: 0.0796 - val_accuracy: 0.9716\n",
            "Epoch 5/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0796 - accuracy: 0.9701 - val_loss: 0.0795 - val_accuracy: 0.9716\n",
            "Epoch 6/250\n",
            "110/110 [==============================] - 10s 91ms/step - loss: 0.0795 - accuracy: 0.9701 - val_loss: 0.0795 - val_accuracy: 0.9716\n",
            "Epoch 7/250\n",
            "110/110 [==============================] - 8s 76ms/step - loss: 0.0795 - accuracy: 0.9702 - val_loss: 0.0795 - val_accuracy: 0.9716\n",
            "Epoch 8/250\n",
            "110/110 [==============================] - 8s 73ms/step - loss: 0.0794 - accuracy: 0.9702 - val_loss: 0.0795 - val_accuracy: 0.9716\n",
            "Epoch 9/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0794 - accuracy: 0.9704 - val_loss: 0.0795 - val_accuracy: 0.9716\n",
            "Epoch 10/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0794 - accuracy: 0.9703 - val_loss: 0.0795 - val_accuracy: 0.9716\n",
            "Epoch 11/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0793 - accuracy: 0.9703 - val_loss: 0.0795 - val_accuracy: 0.9716\n",
            "Epoch 12/250\n",
            "110/110 [==============================] - 9s 81ms/step - loss: 0.0793 - accuracy: 0.9704 - val_loss: 0.0795 - val_accuracy: 0.9716\n",
            "Epoch 13/250\n",
            "110/110 [==============================] - 8s 76ms/step - loss: 0.0793 - accuracy: 0.9704 - val_loss: 0.0795 - val_accuracy: 0.9716\n",
            "Epoch 14/250\n",
            "110/110 [==============================] - 9s 80ms/step - loss: 0.0792 - accuracy: 0.9704 - val_loss: 0.0794 - val_accuracy: 0.9716\n",
            "Epoch 15/250\n",
            "110/110 [==============================] - 9s 87ms/step - loss: 0.0792 - accuracy: 0.9704 - val_loss: 0.0794 - val_accuracy: 0.9716\n",
            "Epoch 16/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0791 - accuracy: 0.9704 - val_loss: 0.0794 - val_accuracy: 0.9716\n",
            "Epoch 17/250\n",
            "110/110 [==============================] - 9s 78ms/step - loss: 0.0791 - accuracy: 0.9706 - val_loss: 0.0794 - val_accuracy: 0.9716\n",
            "Epoch 18/250\n",
            "110/110 [==============================] - 10s 93ms/step - loss: 0.0790 - accuracy: 0.9706 - val_loss: 0.0794 - val_accuracy: 0.9716\n",
            "Epoch 19/250\n",
            "110/110 [==============================] - 8s 74ms/step - loss: 0.0789 - accuracy: 0.9707 - val_loss: 0.0793 - val_accuracy: 0.9717\n",
            "Epoch 20/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0788 - accuracy: 0.9708 - val_loss: 0.0792 - val_accuracy: 0.9717\n",
            "Epoch 21/250\n",
            "110/110 [==============================] - 11s 97ms/step - loss: 0.0787 - accuracy: 0.9709 - val_loss: 0.0791 - val_accuracy: 0.9717\n",
            "Epoch 22/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0786 - accuracy: 0.9709 - val_loss: 0.0790 - val_accuracy: 0.9716\n",
            "Epoch 23/250\n",
            "110/110 [==============================] - 7s 65ms/step - loss: 0.0785 - accuracy: 0.9710 - val_loss: 0.0788 - val_accuracy: 0.9716\n",
            "Epoch 24/250\n",
            "110/110 [==============================] - 11s 101ms/step - loss: 0.0783 - accuracy: 0.9711 - val_loss: 0.0787 - val_accuracy: 0.9716\n",
            "Epoch 25/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0782 - accuracy: 0.9712 - val_loss: 0.0786 - val_accuracy: 0.9718\n",
            "Epoch 26/250\n",
            "110/110 [==============================] - 11s 97ms/step - loss: 0.0781 - accuracy: 0.9713 - val_loss: 0.0785 - val_accuracy: 0.9718\n",
            "Epoch 27/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0780 - accuracy: 0.9713 - val_loss: 0.0784 - val_accuracy: 0.9716\n",
            "Epoch 28/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0778 - accuracy: 0.9713 - val_loss: 0.0783 - val_accuracy: 0.9716\n",
            "Epoch 29/250\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0777 - accuracy: 0.9713 - val_loss: 0.0782 - val_accuracy: 0.9718\n",
            "Epoch 30/250\n",
            "110/110 [==============================] - 10s 92ms/step - loss: 0.0776 - accuracy: 0.9713 - val_loss: 0.0782 - val_accuracy: 0.9718\n",
            "Epoch 31/250\n",
            "110/110 [==============================] - 9s 78ms/step - loss: 0.0774 - accuracy: 0.9714 - val_loss: 0.0780 - val_accuracy: 0.9720\n",
            "Epoch 32/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0773 - accuracy: 0.9713 - val_loss: 0.0779 - val_accuracy: 0.9719\n",
            "Epoch 33/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0771 - accuracy: 0.9715 - val_loss: 0.0778 - val_accuracy: 0.9719\n",
            "Epoch 34/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0769 - accuracy: 0.9715 - val_loss: 0.0776 - val_accuracy: 0.9716\n",
            "Epoch 35/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0766 - accuracy: 0.9716 - val_loss: 0.0774 - val_accuracy: 0.9719\n",
            "Epoch 36/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0762 - accuracy: 0.9720 - val_loss: 0.0769 - val_accuracy: 0.9726\n",
            "Epoch 37/250\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0755 - accuracy: 0.9722 - val_loss: 0.0761 - val_accuracy: 0.9730\n",
            "Epoch 38/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0738 - accuracy: 0.9725 - val_loss: 0.0733 - val_accuracy: 0.9734\n",
            "Epoch 39/250\n",
            "110/110 [==============================] - 9s 81ms/step - loss: 0.0702 - accuracy: 0.9730 - val_loss: 0.0664 - val_accuracy: 0.9744\n",
            "Epoch 40/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0640 - accuracy: 0.9741 - val_loss: 0.0598 - val_accuracy: 0.9759\n",
            "Epoch 41/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0595 - accuracy: 0.9744 - val_loss: 0.0615 - val_accuracy: 0.9749\n",
            "Epoch 42/250\n",
            "110/110 [==============================] - 8s 72ms/step - loss: 0.0575 - accuracy: 0.9746 - val_loss: 0.0565 - val_accuracy: 0.9752\n",
            "Epoch 43/250\n",
            "110/110 [==============================] - 9s 78ms/step - loss: 0.0556 - accuracy: 0.9748 - val_loss: 0.0544 - val_accuracy: 0.9759\n",
            "Epoch 44/250\n",
            "110/110 [==============================] - 10s 93ms/step - loss: 0.0545 - accuracy: 0.9751 - val_loss: 0.0534 - val_accuracy: 0.9755\n",
            "Epoch 45/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0542 - accuracy: 0.9749 - val_loss: 0.0531 - val_accuracy: 0.9755\n",
            "Epoch 46/250\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.0532 - accuracy: 0.9750 - val_loss: 0.0532 - val_accuracy: 0.9757\n",
            "Epoch 47/250\n",
            "110/110 [==============================] - 7s 65ms/step - loss: 0.0529 - accuracy: 0.9747 - val_loss: 0.0527 - val_accuracy: 0.9755\n",
            "Epoch 48/250\n",
            "110/110 [==============================] - 8s 74ms/step - loss: 0.0519 - accuracy: 0.9750 - val_loss: 0.0527 - val_accuracy: 0.9755\n",
            "Epoch 49/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0520 - accuracy: 0.9750 - val_loss: 0.0535 - val_accuracy: 0.9757\n",
            "Epoch 50/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0514 - accuracy: 0.9751 - val_loss: 0.0678 - val_accuracy: 0.9743\n",
            "Epoch 51/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0512 - accuracy: 0.9754 - val_loss: 0.0536 - val_accuracy: 0.9754\n",
            "Epoch 52/250\n",
            "110/110 [==============================] - 8s 75ms/step - loss: 0.0508 - accuracy: 0.9756 - val_loss: 0.0517 - val_accuracy: 0.9759\n",
            "Epoch 53/250\n",
            "110/110 [==============================] - 9s 81ms/step - loss: 0.0506 - accuracy: 0.9756 - val_loss: 0.0515 - val_accuracy: 0.9760\n",
            "Epoch 54/250\n",
            "110/110 [==============================] - 8s 75ms/step - loss: 0.0505 - accuracy: 0.9756 - val_loss: 0.0512 - val_accuracy: 0.9759\n",
            "Epoch 55/250\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.0503 - accuracy: 0.9757 - val_loss: 0.0515 - val_accuracy: 0.9756\n",
            "Epoch 56/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0503 - accuracy: 0.9758 - val_loss: 0.0521 - val_accuracy: 0.9754\n",
            "Epoch 57/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0502 - accuracy: 0.9754 - val_loss: 0.0518 - val_accuracy: 0.9757\n",
            "Epoch 58/250\n",
            "110/110 [==============================] - 8s 71ms/step - loss: 0.0499 - accuracy: 0.9757 - val_loss: 0.0517 - val_accuracy: 0.9758\n",
            "Epoch 59/250\n",
            "110/110 [==============================] - 7s 65ms/step - loss: 0.0499 - accuracy: 0.9759 - val_loss: 0.0521 - val_accuracy: 0.9759\n",
            "Epoch 60/250\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0497 - accuracy: 0.9759 - val_loss: 0.0534 - val_accuracy: 0.9754\n",
            "Epoch 61/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0497 - accuracy: 0.9758 - val_loss: 0.0518 - val_accuracy: 0.9754\n",
            "Epoch 62/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0496 - accuracy: 0.9759 - val_loss: 0.0514 - val_accuracy: 0.9763\n",
            "Epoch 63/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0496 - accuracy: 0.9761 - val_loss: 0.0515 - val_accuracy: 0.9756\n",
            "Epoch 64/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0496 - accuracy: 0.9759 - val_loss: 0.0532 - val_accuracy: 0.9755\n",
            "Epoch 65/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0497 - accuracy: 0.9761 - val_loss: 0.0516 - val_accuracy: 0.9757\n",
            "Epoch 66/250\n",
            "110/110 [==============================] - 11s 104ms/step - loss: 0.0493 - accuracy: 0.9760 - val_loss: 0.0522 - val_accuracy: 0.9753\n",
            "Epoch 67/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0493 - accuracy: 0.9762 - val_loss: 0.0514 - val_accuracy: 0.9757\n",
            "Epoch 68/250\n",
            "110/110 [==============================] - 11s 103ms/step - loss: 0.0493 - accuracy: 0.9762 - val_loss: 0.0521 - val_accuracy: 0.9757\n",
            "Epoch 69/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0492 - accuracy: 0.9761 - val_loss: 0.0536 - val_accuracy: 0.9756\n",
            "Epoch 70/250\n",
            "110/110 [==============================] - 8s 70ms/step - loss: 0.0493 - accuracy: 0.9763 - val_loss: 0.0520 - val_accuracy: 0.9758\n",
            "Epoch 71/250\n",
            "110/110 [==============================] - 9s 79ms/step - loss: 0.0492 - accuracy: 0.9764 - val_loss: 0.0517 - val_accuracy: 0.9756\n",
            "Epoch 72/250\n",
            "110/110 [==============================] - 9s 79ms/step - loss: 0.0491 - accuracy: 0.9762 - val_loss: 0.0516 - val_accuracy: 0.9756\n",
            "Epoch 73/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0491 - accuracy: 0.9764 - val_loss: 0.0521 - val_accuracy: 0.9756\n",
            "Epoch 74/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0489 - accuracy: 0.9764 - val_loss: 0.0517 - val_accuracy: 0.9757\n",
            "Epoch 75/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0489 - accuracy: 0.9765 - val_loss: 0.0518 - val_accuracy: 0.9760\n",
            "Epoch 76/250\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.0489 - accuracy: 0.9767 - val_loss: 0.0527 - val_accuracy: 0.9754\n",
            "Epoch 77/250\n",
            "110/110 [==============================] - 11s 97ms/step - loss: 0.0488 - accuracy: 0.9764 - val_loss: 0.0516 - val_accuracy: 0.9757\n",
            "Epoch 78/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0488 - accuracy: 0.9766 - val_loss: 0.0522 - val_accuracy: 0.9753\n",
            "Epoch 79/250\n",
            "110/110 [==============================] - 8s 71ms/step - loss: 0.0487 - accuracy: 0.9769 - val_loss: 0.0518 - val_accuracy: 0.9758\n",
            "Epoch 80/250\n",
            "110/110 [==============================] - 9s 77ms/step - loss: 0.0488 - accuracy: 0.9766 - val_loss: 0.0521 - val_accuracy: 0.9755\n",
            "Epoch 81/250\n",
            "110/110 [==============================] - 9s 79ms/step - loss: 0.0486 - accuracy: 0.9766 - val_loss: 0.0516 - val_accuracy: 0.9757\n",
            "Epoch 82/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0487 - accuracy: 0.9768 - val_loss: 0.0524 - val_accuracy: 0.9757\n",
            "Epoch 83/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0487 - accuracy: 0.9767 - val_loss: 0.0518 - val_accuracy: 0.9755\n",
            "Epoch 84/250\n",
            "110/110 [==============================] - 8s 72ms/step - loss: 0.0486 - accuracy: 0.9768 - val_loss: 0.0522 - val_accuracy: 0.9757\n",
            "Epoch 85/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0485 - accuracy: 0.9768 - val_loss: 0.0522 - val_accuracy: 0.9757\n",
            "Epoch 86/250\n",
            "110/110 [==============================] - 11s 100ms/step - loss: 0.0485 - accuracy: 0.9768 - val_loss: 0.0522 - val_accuracy: 0.9760\n",
            "Epoch 87/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0484 - accuracy: 0.9770 - val_loss: 0.0515 - val_accuracy: 0.9761\n",
            "Epoch 88/250\n",
            "110/110 [==============================] - 11s 102ms/step - loss: 0.0484 - accuracy: 0.9768 - val_loss: 0.0524 - val_accuracy: 0.9759\n",
            "Epoch 89/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0483 - accuracy: 0.9770 - val_loss: 0.0557 - val_accuracy: 0.9751\n",
            "Epoch 90/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0484 - accuracy: 0.9770 - val_loss: 0.0525 - val_accuracy: 0.9754\n",
            "Epoch 91/250\n",
            "110/110 [==============================] - 9s 80ms/step - loss: 0.0484 - accuracy: 0.9768 - val_loss: 0.0529 - val_accuracy: 0.9758\n",
            "Epoch 92/250\n",
            "110/110 [==============================] - 9s 78ms/step - loss: 0.0485 - accuracy: 0.9768 - val_loss: 0.0517 - val_accuracy: 0.9758\n",
            "Epoch 93/250\n",
            "110/110 [==============================] - 8s 70ms/step - loss: 0.0483 - accuracy: 0.9770 - val_loss: 0.0535 - val_accuracy: 0.9758\n",
            "Epoch 94/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0484 - accuracy: 0.9770 - val_loss: 0.0531 - val_accuracy: 0.9754\n",
            "Epoch 95/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0483 - accuracy: 0.9769 - val_loss: 0.0519 - val_accuracy: 0.9756\n",
            "Epoch 96/250\n",
            "110/110 [==============================] - 8s 69ms/step - loss: 0.0483 - accuracy: 0.9773 - val_loss: 0.0518 - val_accuracy: 0.9758\n",
            "Epoch 97/250\n",
            "110/110 [==============================] - 11s 101ms/step - loss: 0.0481 - accuracy: 0.9770 - val_loss: 0.0535 - val_accuracy: 0.9756\n",
            "Epoch 98/250\n",
            "110/110 [==============================] - 7s 68ms/step - loss: 0.0483 - accuracy: 0.9771 - val_loss: 0.0526 - val_accuracy: 0.9758\n",
            "Epoch 99/250\n",
            "110/110 [==============================] - 10s 95ms/step - loss: 0.0481 - accuracy: 0.9770 - val_loss: 0.0520 - val_accuracy: 0.9760\n",
            "Epoch 100/250\n",
            "110/110 [==============================] - 10s 87ms/step - loss: 0.0480 - accuracy: 0.9770 - val_loss: 0.0534 - val_accuracy: 0.9758\n",
            "Epoch 101/250\n",
            "110/110 [==============================] - 10s 94ms/step - loss: 0.0479 - accuracy: 0.9770 - val_loss: 0.0535 - val_accuracy: 0.9755\n",
            "Epoch 102/250\n",
            "110/110 [==============================] - 10s 87ms/step - loss: 0.0480 - accuracy: 0.9772 - val_loss: 0.0520 - val_accuracy: 0.9754\n",
            "Epoch 103/250\n",
            "110/110 [==============================] - 8s 74ms/step - loss: 0.0480 - accuracy: 0.9772 - val_loss: 0.0514 - val_accuracy: 0.9757\n",
            "Epoch 104/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0479 - accuracy: 0.9776 - val_loss: 0.0522 - val_accuracy: 0.9755\n",
            "Epoch 105/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0479 - accuracy: 0.9774 - val_loss: 0.0518 - val_accuracy: 0.9755\n",
            "Epoch 106/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0479 - accuracy: 0.9773 - val_loss: 0.0514 - val_accuracy: 0.9754\n",
            "Epoch 107/250\n",
            "110/110 [==============================] - 10s 86ms/step - loss: 0.0481 - accuracy: 0.9771 - val_loss: 0.0521 - val_accuracy: 0.9754\n",
            "Epoch 108/250\n",
            "110/110 [==============================] - 8s 76ms/step - loss: 0.0478 - accuracy: 0.9775 - val_loss: 0.0519 - val_accuracy: 0.9755\n",
            "Epoch 109/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0477 - accuracy: 0.9774 - val_loss: 0.0523 - val_accuracy: 0.9756\n",
            "Epoch 110/250\n",
            "110/110 [==============================] - 11s 105ms/step - loss: 0.0477 - accuracy: 0.9772 - val_loss: 0.0520 - val_accuracy: 0.9754\n",
            "Epoch 111/250\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0476 - accuracy: 0.9774 - val_loss: 0.0524 - val_accuracy: 0.9756\n",
            "Epoch 112/250\n",
            "110/110 [==============================] - 12s 107ms/step - loss: 0.0476 - accuracy: 0.9773 - val_loss: 0.0535 - val_accuracy: 0.9752\n",
            "Epoch 113/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0475 - accuracy: 0.9775 - val_loss: 0.0527 - val_accuracy: 0.9750\n",
            "Epoch 114/250\n",
            "110/110 [==============================] - 9s 78ms/step - loss: 0.0474 - accuracy: 0.9776 - val_loss: 0.0523 - val_accuracy: 0.9753\n",
            "Epoch 115/250\n",
            "110/110 [==============================] - 12s 106ms/step - loss: 0.0475 - accuracy: 0.9773 - val_loss: 0.0526 - val_accuracy: 0.9756\n",
            "Epoch 116/250\n",
            "110/110 [==============================] - 8s 73ms/step - loss: 0.0476 - accuracy: 0.9778 - val_loss: 0.0526 - val_accuracy: 0.9755\n",
            "Epoch 117/250\n",
            "110/110 [==============================] - 8s 70ms/step - loss: 0.0475 - accuracy: 0.9776 - val_loss: 0.0523 - val_accuracy: 0.9759\n",
            "Epoch 118/250\n",
            "110/110 [==============================] - 12s 112ms/step - loss: 0.0474 - accuracy: 0.9776 - val_loss: 0.0522 - val_accuracy: 0.9757\n",
            "Epoch 119/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0472 - accuracy: 0.9777 - val_loss: 0.0524 - val_accuracy: 0.9755\n",
            "Epoch 120/250\n",
            "110/110 [==============================] - 12s 105ms/step - loss: 0.0474 - accuracy: 0.9776 - val_loss: 0.0527 - val_accuracy: 0.9754\n",
            "Epoch 121/250\n",
            "110/110 [==============================] - 13s 121ms/step - loss: 0.0474 - accuracy: 0.9777 - val_loss: 0.0523 - val_accuracy: 0.9754\n",
            "Epoch 122/250\n",
            "110/110 [==============================] - 12s 105ms/step - loss: 0.0471 - accuracy: 0.9776 - val_loss: 0.0529 - val_accuracy: 0.9751\n",
            "Epoch 123/250\n",
            "110/110 [==============================] - 10s 94ms/step - loss: 0.0472 - accuracy: 0.9775 - val_loss: 0.0527 - val_accuracy: 0.9751\n",
            "Epoch 124/250\n",
            "110/110 [==============================] - 8s 73ms/step - loss: 0.0471 - accuracy: 0.9778 - val_loss: 0.0535 - val_accuracy: 0.9752\n",
            "Epoch 125/250\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0472 - accuracy: 0.9777 - val_loss: 0.0527 - val_accuracy: 0.9753\n",
            "Epoch 126/250\n",
            "110/110 [==============================] - 11s 98ms/step - loss: 0.0470 - accuracy: 0.9778 - val_loss: 0.0533 - val_accuracy: 0.9753\n",
            "Epoch 127/250\n",
            "110/110 [==============================] - 9s 80ms/step - loss: 0.0471 - accuracy: 0.9778 - val_loss: 0.0530 - val_accuracy: 0.9753\n",
            "Epoch 128/250\n",
            "110/110 [==============================] - 8s 70ms/step - loss: 0.0467 - accuracy: 0.9779 - val_loss: 0.0538 - val_accuracy: 0.9750\n",
            "Epoch 129/250\n",
            "110/110 [==============================] - 10s 92ms/step - loss: 0.0469 - accuracy: 0.9778 - val_loss: 0.0543 - val_accuracy: 0.9752\n",
            "Epoch 130/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0469 - accuracy: 0.9779 - val_loss: 0.0537 - val_accuracy: 0.9752\n",
            "Epoch 131/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0466 - accuracy: 0.9779 - val_loss: 0.0539 - val_accuracy: 0.9748\n",
            "Epoch 132/250\n",
            "110/110 [==============================] - 11s 100ms/step - loss: 0.0467 - accuracy: 0.9781 - val_loss: 0.0543 - val_accuracy: 0.9750\n",
            "Epoch 133/250\n",
            "110/110 [==============================] - 8s 68ms/step - loss: 0.0466 - accuracy: 0.9781 - val_loss: 0.0545 - val_accuracy: 0.9748\n",
            "Epoch 134/250\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.0466 - accuracy: 0.9781 - val_loss: 0.0541 - val_accuracy: 0.9750\n",
            "Epoch 135/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0465 - accuracy: 0.9779 - val_loss: 0.0547 - val_accuracy: 0.9748\n",
            "Epoch 136/250\n",
            "110/110 [==============================] - 9s 77ms/step - loss: 0.0465 - accuracy: 0.9780 - val_loss: 0.0548 - val_accuracy: 0.9751\n",
            "Epoch 137/250\n",
            "110/110 [==============================] - 9s 80ms/step - loss: 0.0464 - accuracy: 0.9781 - val_loss: 0.0551 - val_accuracy: 0.9750\n",
            "Epoch 138/250\n",
            "110/110 [==============================] - 9s 80ms/step - loss: 0.0463 - accuracy: 0.9783 - val_loss: 0.0555 - val_accuracy: 0.9750\n",
            "Epoch 139/250\n",
            "110/110 [==============================] - 10s 91ms/step - loss: 0.0462 - accuracy: 0.9782 - val_loss: 0.0541 - val_accuracy: 0.9749\n",
            "Epoch 140/250\n",
            "110/110 [==============================] - 8s 74ms/step - loss: 0.0462 - accuracy: 0.9782 - val_loss: 0.0545 - val_accuracy: 0.9747\n",
            "Epoch 141/250\n",
            "110/110 [==============================] - 9s 77ms/step - loss: 0.0464 - accuracy: 0.9783 - val_loss: 0.0552 - val_accuracy: 0.9744\n",
            "Epoch 142/250\n",
            "110/110 [==============================] - 12s 108ms/step - loss: 0.0462 - accuracy: 0.9783 - val_loss: 0.0556 - val_accuracy: 0.9746\n",
            "Epoch 143/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0460 - accuracy: 0.9785 - val_loss: 0.0554 - val_accuracy: 0.9746\n",
            "Epoch 144/250\n",
            "110/110 [==============================] - 11s 101ms/step - loss: 0.0461 - accuracy: 0.9785 - val_loss: 0.0547 - val_accuracy: 0.9748\n",
            "Epoch 145/250\n",
            "110/110 [==============================] - 10s 92ms/step - loss: 0.0458 - accuracy: 0.9784 - val_loss: 0.0563 - val_accuracy: 0.9745\n",
            "Epoch 146/250\n",
            "110/110 [==============================] - 10s 95ms/step - loss: 0.0457 - accuracy: 0.9787 - val_loss: 0.0557 - val_accuracy: 0.9744\n",
            "Epoch 147/250\n",
            "110/110 [==============================] - 7s 68ms/step - loss: 0.0460 - accuracy: 0.9787 - val_loss: 0.0557 - val_accuracy: 0.9749\n",
            "Epoch 148/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0455 - accuracy: 0.9788 - val_loss: 0.0562 - val_accuracy: 0.9751\n",
            "Epoch 149/250\n",
            "110/110 [==============================] - 11s 97ms/step - loss: 0.0455 - accuracy: 0.9788 - val_loss: 0.0564 - val_accuracy: 0.9746\n",
            "Epoch 150/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0456 - accuracy: 0.9787 - val_loss: 0.0570 - val_accuracy: 0.9749\n",
            "Epoch 151/250\n",
            "110/110 [==============================] - 8s 76ms/step - loss: 0.0454 - accuracy: 0.9790 - val_loss: 0.0569 - val_accuracy: 0.9746\n",
            "Epoch 152/250\n",
            "110/110 [==============================] - 13s 117ms/step - loss: 0.0455 - accuracy: 0.9790 - val_loss: 0.0566 - val_accuracy: 0.9748\n",
            "Epoch 153/250\n",
            "110/110 [==============================] - 13s 119ms/step - loss: 0.0453 - accuracy: 0.9791 - val_loss: 0.0567 - val_accuracy: 0.9749\n",
            "Epoch 154/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0452 - accuracy: 0.9793 - val_loss: 0.0573 - val_accuracy: 0.9748\n",
            "Epoch 155/250\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.0450 - accuracy: 0.9792 - val_loss: 0.0566 - val_accuracy: 0.9749\n",
            "Epoch 156/250\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.0450 - accuracy: 0.9794 - val_loss: 0.0580 - val_accuracy: 0.9751\n",
            "Epoch 157/250\n",
            "110/110 [==============================] - 8s 75ms/step - loss: 0.0449 - accuracy: 0.9794 - val_loss: 0.0566 - val_accuracy: 0.9747\n",
            "Epoch 158/250\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.0447 - accuracy: 0.9795 - val_loss: 0.0582 - val_accuracy: 0.9747\n",
            "Epoch 159/250\n",
            "110/110 [==============================] - 11s 99ms/step - loss: 0.0447 - accuracy: 0.9796 - val_loss: 0.0577 - val_accuracy: 0.9752\n",
            "Epoch 160/250\n",
            "110/110 [==============================] - 8s 68ms/step - loss: 0.0448 - accuracy: 0.9795 - val_loss: 0.0605 - val_accuracy: 0.9743\n",
            "Epoch 161/250\n",
            "110/110 [==============================] - 11s 96ms/step - loss: 0.0445 - accuracy: 0.9798 - val_loss: 0.0586 - val_accuracy: 0.9750\n",
            "Epoch 162/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0446 - accuracy: 0.9797 - val_loss: 0.0579 - val_accuracy: 0.9750\n",
            "Epoch 163/250\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.0443 - accuracy: 0.9801 - val_loss: 0.0599 - val_accuracy: 0.9747\n",
            "Epoch 164/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0443 - accuracy: 0.9799 - val_loss: 0.0589 - val_accuracy: 0.9750\n",
            "Epoch 165/250\n",
            "110/110 [==============================] - 8s 74ms/step - loss: 0.0441 - accuracy: 0.9801 - val_loss: 0.0584 - val_accuracy: 0.9746\n",
            "Epoch 166/250\n",
            "110/110 [==============================] - 11s 98ms/step - loss: 0.0440 - accuracy: 0.9804 - val_loss: 0.0599 - val_accuracy: 0.9748\n",
            "Epoch 167/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0441 - accuracy: 0.9802 - val_loss: 0.0602 - val_accuracy: 0.9749\n",
            "Epoch 168/250\n",
            "110/110 [==============================] - 8s 68ms/step - loss: 0.0437 - accuracy: 0.9802 - val_loss: 0.0612 - val_accuracy: 0.9746\n",
            "Epoch 169/250\n",
            "110/110 [==============================] - 10s 93ms/step - loss: 0.0438 - accuracy: 0.9804 - val_loss: 0.0597 - val_accuracy: 0.9746\n",
            "Epoch 170/250\n",
            "110/110 [==============================] - 8s 75ms/step - loss: 0.0439 - accuracy: 0.9803 - val_loss: 0.0612 - val_accuracy: 0.9744\n",
            "Epoch 171/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0435 - accuracy: 0.9806 - val_loss: 0.0637 - val_accuracy: 0.9744\n",
            "Epoch 172/250\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0436 - accuracy: 0.9807 - val_loss: 0.0611 - val_accuracy: 0.9744\n",
            "Epoch 173/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0435 - accuracy: 0.9807 - val_loss: 0.0600 - val_accuracy: 0.9746\n",
            "Epoch 174/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0432 - accuracy: 0.9808 - val_loss: 0.0613 - val_accuracy: 0.9744\n",
            "Epoch 175/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0430 - accuracy: 0.9809 - val_loss: 0.0614 - val_accuracy: 0.9744\n",
            "Epoch 176/250\n",
            "110/110 [==============================] - 7s 68ms/step - loss: 0.0431 - accuracy: 0.9807 - val_loss: 0.0606 - val_accuracy: 0.9749\n",
            "Epoch 177/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0427 - accuracy: 0.9812 - val_loss: 0.0617 - val_accuracy: 0.9748\n",
            "Epoch 178/250\n",
            "110/110 [==============================] - 10s 94ms/step - loss: 0.0426 - accuracy: 0.9811 - val_loss: 0.0603 - val_accuracy: 0.9753\n",
            "Epoch 179/250\n",
            "110/110 [==============================] - 9s 79ms/step - loss: 0.0427 - accuracy: 0.9809 - val_loss: 0.0625 - val_accuracy: 0.9743\n",
            "Epoch 180/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0425 - accuracy: 0.9812 - val_loss: 0.0635 - val_accuracy: 0.9743\n",
            "Epoch 181/250\n",
            "110/110 [==============================] - 7s 67ms/step - loss: 0.0421 - accuracy: 0.9813 - val_loss: 0.0642 - val_accuracy: 0.9746\n",
            "Epoch 182/250\n",
            "110/110 [==============================] - 10s 90ms/step - loss: 0.0420 - accuracy: 0.9813 - val_loss: 0.0662 - val_accuracy: 0.9743\n",
            "Epoch 183/250\n",
            "110/110 [==============================] - 8s 77ms/step - loss: 0.0420 - accuracy: 0.9813 - val_loss: 0.0642 - val_accuracy: 0.9749\n",
            "Epoch 184/250\n",
            "110/110 [==============================] - 8s 74ms/step - loss: 0.0418 - accuracy: 0.9815 - val_loss: 0.0627 - val_accuracy: 0.9748\n",
            "Epoch 185/250\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.0424 - accuracy: 0.9812 - val_loss: 0.0626 - val_accuracy: 0.9748\n",
            "Epoch 186/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0419 - accuracy: 0.9815 - val_loss: 0.0665 - val_accuracy: 0.9741\n",
            "Epoch 187/250\n",
            "110/110 [==============================] - 8s 75ms/step - loss: 0.0418 - accuracy: 0.9818 - val_loss: 0.0629 - val_accuracy: 0.9749\n",
            "Epoch 188/250\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.0417 - accuracy: 0.9816 - val_loss: 0.0666 - val_accuracy: 0.9750\n",
            "Epoch 189/250\n",
            "110/110 [==============================] - 7s 68ms/step - loss: 0.0414 - accuracy: 0.9817 - val_loss: 0.0685 - val_accuracy: 0.9743\n",
            "Epoch 190/250\n",
            "110/110 [==============================] - 8s 77ms/step - loss: 0.0415 - accuracy: 0.9817 - val_loss: 0.0670 - val_accuracy: 0.9749\n",
            "Epoch 191/250\n",
            "110/110 [==============================] - 11s 99ms/step - loss: 0.0413 - accuracy: 0.9819 - val_loss: 0.0665 - val_accuracy: 0.9745\n",
            "Epoch 192/250\n",
            "110/110 [==============================] - 11s 100ms/step - loss: 0.0410 - accuracy: 0.9822 - val_loss: 0.0691 - val_accuracy: 0.9747\n",
            "Epoch 193/250\n",
            "110/110 [==============================] - 17s 157ms/step - loss: 0.0408 - accuracy: 0.9822 - val_loss: 0.0649 - val_accuracy: 0.9751\n",
            "Epoch 194/250\n",
            "110/110 [==============================] - 8s 75ms/step - loss: 0.0409 - accuracy: 0.9820 - val_loss: 0.0703 - val_accuracy: 0.9747\n",
            "Epoch 195/250\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.0409 - accuracy: 0.9821 - val_loss: 0.0683 - val_accuracy: 0.9748\n",
            "Epoch 196/250\n",
            "110/110 [==============================] - 10s 92ms/step - loss: 0.0403 - accuracy: 0.9826 - val_loss: 0.0715 - val_accuracy: 0.9745\n",
            "Epoch 197/250\n",
            "110/110 [==============================] - 8s 74ms/step - loss: 0.0405 - accuracy: 0.9825 - val_loss: 0.0722 - val_accuracy: 0.9744\n",
            "Epoch 198/250\n",
            "110/110 [==============================] - 12s 106ms/step - loss: 0.0403 - accuracy: 0.9824 - val_loss: 0.0689 - val_accuracy: 0.9746\n",
            "Epoch 199/250\n",
            "110/110 [==============================] - 8s 70ms/step - loss: 0.0401 - accuracy: 0.9825 - val_loss: 0.0729 - val_accuracy: 0.9746\n",
            "Epoch 200/250\n",
            "110/110 [==============================] - 10s 87ms/step - loss: 0.0400 - accuracy: 0.9828 - val_loss: 0.0699 - val_accuracy: 0.9748\n",
            "Epoch 201/250\n",
            "110/110 [==============================] - 12s 107ms/step - loss: 0.0398 - accuracy: 0.9829 - val_loss: 0.0701 - val_accuracy: 0.9748\n",
            "Epoch 202/250\n",
            "110/110 [==============================] - 8s 69ms/step - loss: 0.0398 - accuracy: 0.9829 - val_loss: 0.0718 - val_accuracy: 0.9747\n",
            "Epoch 203/250\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0395 - accuracy: 0.9831 - val_loss: 0.0699 - val_accuracy: 0.9745\n",
            "Epoch 204/250\n",
            "110/110 [==============================] - 10s 94ms/step - loss: 0.0394 - accuracy: 0.9830 - val_loss: 0.0719 - val_accuracy: 0.9746\n",
            "Epoch 205/250\n",
            "110/110 [==============================] - 8s 69ms/step - loss: 0.0395 - accuracy: 0.9827 - val_loss: 0.0723 - val_accuracy: 0.9747\n",
            "Epoch 206/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0391 - accuracy: 0.9829 - val_loss: 0.0715 - val_accuracy: 0.9749\n",
            "Epoch 207/250\n",
            "110/110 [==============================] - 10s 89ms/step - loss: 0.0388 - accuracy: 0.9832 - val_loss: 0.0715 - val_accuracy: 0.9747\n",
            "Epoch 208/250\n",
            "110/110 [==============================] - 10s 92ms/step - loss: 0.0388 - accuracy: 0.9831 - val_loss: 0.0702 - val_accuracy: 0.9748\n",
            "Epoch 209/250\n",
            "110/110 [==============================] - 12s 112ms/step - loss: 0.0386 - accuracy: 0.9830 - val_loss: 0.0718 - val_accuracy: 0.9746\n",
            "Epoch 210/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0384 - accuracy: 0.9832 - val_loss: 0.0699 - val_accuracy: 0.9754\n",
            "Epoch 211/250\n",
            "110/110 [==============================] - 9s 79ms/step - loss: 0.0384 - accuracy: 0.9832 - val_loss: 0.0703 - val_accuracy: 0.9749\n",
            "Epoch 212/250\n",
            "110/110 [==============================] - 10s 87ms/step - loss: 0.0381 - accuracy: 0.9834 - val_loss: 0.0704 - val_accuracy: 0.9752\n",
            "Epoch 213/250\n",
            "110/110 [==============================] - 9s 81ms/step - loss: 0.0380 - accuracy: 0.9833 - val_loss: 0.0742 - val_accuracy: 0.9746\n",
            "Epoch 214/250\n",
            "110/110 [==============================] - 9s 81ms/step - loss: 0.0377 - accuracy: 0.9836 - val_loss: 0.0733 - val_accuracy: 0.9745\n",
            "Epoch 215/250\n",
            "110/110 [==============================] - 9s 82ms/step - loss: 0.0375 - accuracy: 0.9837 - val_loss: 0.0747 - val_accuracy: 0.9746\n",
            "Epoch 216/250\n",
            "110/110 [==============================] - 9s 78ms/step - loss: 0.0373 - accuracy: 0.9838 - val_loss: 0.0742 - val_accuracy: 0.9744\n",
            "Epoch 217/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0370 - accuracy: 0.9840 - val_loss: 0.0763 - val_accuracy: 0.9745\n",
            "Epoch 218/250\n",
            "110/110 [==============================] - 8s 73ms/step - loss: 0.0370 - accuracy: 0.9839 - val_loss: 0.0759 - val_accuracy: 0.9745\n",
            "Epoch 219/250\n",
            "110/110 [==============================] - 12s 109ms/step - loss: 0.0371 - accuracy: 0.9842 - val_loss: 0.0753 - val_accuracy: 0.9744\n",
            "Epoch 220/250\n",
            "110/110 [==============================] - 10s 87ms/step - loss: 0.0367 - accuracy: 0.9841 - val_loss: 0.0730 - val_accuracy: 0.9747\n",
            "Epoch 221/250\n",
            "110/110 [==============================] - 11s 97ms/step - loss: 0.0367 - accuracy: 0.9842 - val_loss: 0.0743 - val_accuracy: 0.9753\n",
            "Epoch 222/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0367 - accuracy: 0.9843 - val_loss: 0.0755 - val_accuracy: 0.9749\n",
            "Epoch 223/250\n",
            "110/110 [==============================] - 14s 128ms/step - loss: 0.0363 - accuracy: 0.9843 - val_loss: 0.0761 - val_accuracy: 0.9754\n",
            "Epoch 224/250\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.0367 - accuracy: 0.9844 - val_loss: 0.0746 - val_accuracy: 0.9748\n",
            "Epoch 225/250\n",
            "110/110 [==============================] - 7s 68ms/step - loss: 0.0364 - accuracy: 0.9844 - val_loss: 0.0749 - val_accuracy: 0.9749\n",
            "Epoch 226/250\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.0362 - accuracy: 0.9845 - val_loss: 0.0755 - val_accuracy: 0.9743\n",
            "Epoch 227/250\n",
            "110/110 [==============================] - 11s 99ms/step - loss: 0.0363 - accuracy: 0.9848 - val_loss: 0.0755 - val_accuracy: 0.9749\n",
            "Epoch 228/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0360 - accuracy: 0.9848 - val_loss: 0.0759 - val_accuracy: 0.9751\n",
            "Epoch 229/250\n",
            "110/110 [==============================] - 9s 79ms/step - loss: 0.0356 - accuracy: 0.9848 - val_loss: 0.0770 - val_accuracy: 0.9749\n",
            "Epoch 230/250\n",
            "110/110 [==============================] - 8s 68ms/step - loss: 0.0354 - accuracy: 0.9849 - val_loss: 0.0793 - val_accuracy: 0.9746\n",
            "Epoch 231/250\n",
            "110/110 [==============================] - 10s 95ms/step - loss: 0.0355 - accuracy: 0.9851 - val_loss: 0.0765 - val_accuracy: 0.9750\n",
            "Epoch 232/250\n",
            "110/110 [==============================] - 10s 90ms/step - loss: 0.0355 - accuracy: 0.9850 - val_loss: 0.0785 - val_accuracy: 0.9749\n",
            "Epoch 233/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0351 - accuracy: 0.9851 - val_loss: 0.0791 - val_accuracy: 0.9747\n",
            "Epoch 234/250\n",
            "110/110 [==============================] - 12s 108ms/step - loss: 0.0347 - accuracy: 0.9855 - val_loss: 0.0794 - val_accuracy: 0.9746\n",
            "Epoch 235/250\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.0346 - accuracy: 0.9854 - val_loss: 0.0810 - val_accuracy: 0.9746\n",
            "Epoch 236/250\n",
            "110/110 [==============================] - 7s 68ms/step - loss: 0.0348 - accuracy: 0.9854 - val_loss: 0.0792 - val_accuracy: 0.9750\n",
            "Epoch 237/250\n",
            "110/110 [==============================] - 10s 87ms/step - loss: 0.0343 - accuracy: 0.9855 - val_loss: 0.0797 - val_accuracy: 0.9750\n",
            "Epoch 238/250\n",
            "110/110 [==============================] - 8s 77ms/step - loss: 0.0340 - accuracy: 0.9857 - val_loss: 0.0804 - val_accuracy: 0.9747\n",
            "Epoch 239/250\n",
            "110/110 [==============================] - 11s 101ms/step - loss: 0.0338 - accuracy: 0.9859 - val_loss: 0.0797 - val_accuracy: 0.9742\n",
            "Epoch 240/250\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.0338 - accuracy: 0.9858 - val_loss: 0.0779 - val_accuracy: 0.9747\n",
            "Epoch 241/250\n",
            "110/110 [==============================] - 7s 66ms/step - loss: 0.0337 - accuracy: 0.9859 - val_loss: 0.0799 - val_accuracy: 0.9749\n",
            "Epoch 242/250\n",
            "110/110 [==============================] - 12s 105ms/step - loss: 0.0331 - accuracy: 0.9863 - val_loss: 0.0828 - val_accuracy: 0.9746\n",
            "Epoch 243/250\n",
            "110/110 [==============================] - 8s 68ms/step - loss: 0.0333 - accuracy: 0.9862 - val_loss: 0.0814 - val_accuracy: 0.9743\n",
            "Epoch 244/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0329 - accuracy: 0.9862 - val_loss: 0.0813 - val_accuracy: 0.9749\n",
            "Epoch 245/250\n",
            "110/110 [==============================] - 11s 98ms/step - loss: 0.0331 - accuracy: 0.9861 - val_loss: 0.0823 - val_accuracy: 0.9745\n",
            "Epoch 246/250\n",
            "110/110 [==============================] - 8s 74ms/step - loss: 0.0329 - accuracy: 0.9864 - val_loss: 0.0820 - val_accuracy: 0.9744\n",
            "Epoch 247/250\n",
            "110/110 [==============================] - 9s 85ms/step - loss: 0.0329 - accuracy: 0.9862 - val_loss: 0.0824 - val_accuracy: 0.9746\n",
            "Epoch 248/250\n",
            "110/110 [==============================] - 11s 98ms/step - loss: 0.0322 - accuracy: 0.9866 - val_loss: 0.0826 - val_accuracy: 0.9749\n",
            "Epoch 249/250\n",
            "110/110 [==============================] - 9s 86ms/step - loss: 0.0322 - accuracy: 0.9869 - val_loss: 0.0822 - val_accuracy: 0.9744\n",
            "Epoch 250/250\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.0321 - accuracy: 0.9868 - val_loss: 0.0818 - val_accuracy: 0.9742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f25bca21e10>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.fit(dataset1, \n",
        "            epochs= 250,\n",
        "            validation_data=dataset_validation\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95NsSBxqJ6tH"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/DL/DL Project 3/vandy/project3_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbtZpSN0GA7p"
      },
      "outputs": [],
      "source": [
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def nextword_generation(sentence,temperature):\n",
        "  generate_length=1\n",
        "  length=len(sentence[0].split(\" \"))\n",
        "  tokenized_sentence = text_vectorization([sentence])\n",
        "  predictions = model(tokenized_sentence)\n",
        "  next_token = sample_next(predictions[0, 0, :],temperature)\n",
        "  sampled_token = idx_token[next_token]\n",
        "  return sampled_token\n",
        "\n",
        "\n",
        "test_data_ip_str = [pair[0] for pair in test_pairs]\n",
        "test_target_texts = [pair[1] for pair in test_pairs]\n",
        "decoded_nextwords=[]\n",
        "for input_sentence in test_data_ip_str:\n",
        "  sampled_token=nextword_generation(input_sentence,temperature=1)\n",
        "  decoded_nextwords.append(sampled_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8MNQBjpE-0h"
      },
      "outputs": [],
      "source": [
        "def findAccuracy(test_input,test_decoded_sentences):\n",
        "    count = 0\n",
        "    total_count=len(test_input)\n",
        "    for i in range(total_count):\n",
        "        if test_input[i] == test_decoded_sentences[i]:\n",
        "            count += 1\n",
        "    print(count,total_count)\n",
        "    accuracy=count/total_count*100\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy=findAccuracy(test_target_texts,decoded_nextwords) \n",
        "print(\"accuracy\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRH6VVxluL8M",
        "outputId": "932d2037-eab9-4435-a815-9fae168fbd8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "213 750\n",
            "accuracy 28.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test data output**"
      ],
      "metadata": {
        "id": "qKUwkJiprN-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=pickle.load(open('/content/drive/MyDrive/DL_Project3/DS_6_test_input_prefixList','rb'))"
      ],
      "metadata": {
        "id": "pXATa_e_u3nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_str=[]\n",
        "for i in test_data:\n",
        "  test_data_str.append([\" \".join(i)])\n",
        "\n",
        "decoded_nextwords_final=[]\n",
        "for input_sentence in test_data_str:\n",
        "  sampled_token=nextword_generation(input_sentence,temperature=1)\n",
        "  decoded_nextwords_final.append(sampled_token)\n",
        "\n",
        "pickle.dump(decoded_nextwords_final, open(\n",
        "  \"/content/drive/MyDrive/DL/DL Project 3/Untitled Folder/ActualTestDataOutputList.pkl\",'wb'))"
      ],
      "metadata": {
        "id": "Yj0QqG3W0swM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}